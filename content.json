[{"title":"Batch normalization学习","date":"2019-11-18T15:35:00.000Z","path":"2019/11/18/Batch normalization学习/","text":"参考文献： Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift Batch Normalization（笔记整理） Batch normalization理解 关于BatchNormalization的讨论 深度学习中 Batch Normalization为什么效果好？ Batch Normalization原理与实战 为什么有Batch Normalization？ 数据分布不稳定，为了解决internal covariate shift(在训练过程中深度网络内部隐层分布的不一致)。机器学习领域的假设——独立同分布，即假设训练数据和测试数据是满足相同分布的，希望通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障(不是必然要求，但可以简化常规机器学习模型的训练，提升机器学习模型的预测能力)。而深度神经网络中，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。 网络的训练，需要去学习适应不同的数据分布，明显造成的后果就是收敛慢，效果不佳。 在数据预处理中进行“白化”操作(LeCun et al.,1998b；Wiesler&amp;Ney,2011)，一个是去除特征之间的相关性，也即保持数据的独立性，另一个是使得所有特征具有相同的均值和方差，也即保持数据的同分布。最典型的方法是PCA。但白化主要有以下问题： 白化过程计算成本太高，并且在每一轮训练中的每一层我们都需要做如此高成本计算的白化操作；白化过程由于改变了网络每一层的分布，因而改变了网络层中本身数据的表达能力。底层网络学习到的参数信息会被白化操作丢失掉。 梯度弥散，减缓网络收敛速度随着模型训练的进行，输入的变化可能整体趋向于变大或变小，导致BP时梯度爆炸或梯度消失，容易陷入饱和区域 参数调整网络前面的参数变更会随着网络深度，增大对后面数据输入的影响，因此每层的参数更新策略需要尽可能的谨慎。 用在哪？在网络的每一层输入之前，做一个归一化处理，即$bn(wu+b)$，然后再接激活函数。(来源3)可以应用于网络中任意的 activation set？？(来源5—— 魏秀参 )在多层CNN里，BN放在卷积层之后，激活和池化之前，以LeNet5为例 。(来源2) 什么时候用？在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。（来源5—— 魏秀参） 作用是什么？ 防止梯度弥散,缓解梯度饱和。一种方法是采用非饱和性激活函数如ReLU,而另一种方法针对sigmoid和tanh激活函数，他们存在饱和区域(即越接近两边，梯度会接近0)， 具体以sigmoid为例， z = g(wu+b)，其中u为输入层，w和b为权重矩阵和偏移向量（即网络层里需要学习的参数）$$ g(x) = \\dfrac{1}{1+e^{-x}} $$ $Sigmoid’=sigmoid*(1-sigmoid)$ 。随着|x|的增加，g’(x)会趋向于0，这意味着，在所有维数上，除了那些绝对值较小的 x=wu+b ，下降到 u 的梯度将消失，模型将缓慢训练。而x受W,b和前面所有层参数的影响。 使每一层神经网络的输入保持相同分布。 IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。 好处：1.没有它之前，需要小心的调整学习率和权重初始化，但是有了BN可以放心的使用大学习率；2.极大提升了训练速度，收敛过程大大加快3.减轻了对参数初始化的依赖4.Batch Normalization本身上也是一种正则的方式，一定程度上增加了泛化能力，可以代替其他正则方式如Dropout等。 缺陷：1.当batch尺寸很小时，计算的均值和方差不稳定。不适应于当训练资源有限而无法应用较大的batch的场景。2.动态的神经网络，如rnn中没效果。3.训练数据集和测试数据集方差比较大的时候。(来源4) 为什么有效：(1) 主流观点，Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，目前大部分资料都这样解释，比如BN的原始论文认为的缓解了Internal Covariate Shift(ICS)问题。 (2) 可以使用更大的学习率，文[2]指出BN有效是因为用上BN层之后可以使用更大的学习率，从而跳出不好的局部极值，增强泛化能力，在它们的研究中做了大量的实验来验证。 (3) 损失平面平滑。文[3]的研究提出，BN有效的根本原因不在于调整了分布，因为即使是在BN层后模拟ICS，也仍然可以取得好的结果。它们指出，BN有效的根本原因是平滑了损失平面。之前我们说过，Z-score标准化对于包括孤立点的分布可以进行更平滑的调整。[1805.11604]How Does Batch Normalization Help Optimization认为BN与ICS无关，并不能减少ICS。（来源5—— 言有三-龙鹏 ） CNN中的BN如何操作(来源4) 我们知道，常规的CNN一般由卷积层、下采样层及全连接层构成。全连接层形式上与前向神经网络是一样的，所以可以采取前向神经网络中的BatchNorm方式，而下采样层本身不带参数所以可以忽略，所以CNN中主要关注卷积层如何计算BatchNorm。 CNN中的某个卷积层由m个卷积核构成，每个卷积核对三维的输入（通道数*长*宽）进行计算，激活及输出值是个二维平面（长*宽），对应一个输出通道（参考下面第一幅图），由于存在m个卷积核，所以输出仍然是三维的，由m个通道及每个通道的二维平面构成。 那么在卷积层中，如果要对通道激活二维平面中某个激活值进行Normalization操作，怎么确定神经元集合S的势力范围呢？第二幅图给出了示意图。类似于前向神经网络中的BatchNorm计算过程，对于Mini-Batch训练方法来说，反向传播更新梯度使用Batch中所有实例的梯度方向来进行。 ​ 所以对于CNN某个卷积层对应的输出通道k来说，假设某个Batch包含n个训练实例，那么每个训练实例在这个通道k都会产生一个二维激活平面，也就是说Batch中n个训练实例分别通过同一个卷积核的输出通道k的时候产生了n个激活平面(Feature Map)。假设激活平面长为5，宽为4，则激活平面包含20个激活值，n个不同实例的激活平面共包含20*n个激活值。 ​ 那么BatchNorm的集合S的范围就是由这20*n个同一个通道被Batch不同训练实例激发的激活平面中包含的所有激活值构成（对应图8中所有标为蓝色的激活值）。划定集合S的范围后，激活平面中任意一个激活值都需进行Normalization操作，其Normalization的具体计算过程与前文所述计算过程一样，采用公式3即可完成规范化操作。这样即完成CNN卷积层的BatchNorm转换过程。 其中， 为某个神经元原始激活值，为经过规范化操作后的规范后值 ,$\\mu$自己认为是20*n个激活值的平均值，$\\sigma$是4x5xn个统计数据产生，而每个特征图使用一组$\\gamma$和$\\beta$。 在下图中,假设一个批量有个样本，Feature Map的尺寸是pxq，通道数是，在卷积网络的中，BN的操作是以Feature Map为单位的，因此一个BN要统计的数据个数为 mxpxq，每个Feature Map使用一组 和。 公式理解：$$ \\hat{x}^{k} = \\dfrac{x^{(K)}-E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}} $$ 其中$E[x]$为均值，$\\sqrt{Var}$为标准差，接下来再$$y^{(k)} = \\gamma^{(k)}\\hat{x}^{(k)}+\\beta^{(k)}$$ $y^{(k)}$就是$bn()$的输出批处理的公式流程如下数学上理解，激活函数的输入$x=wu+b$会随着网络加深，逐渐向两端靠拢，两个函数的导数在两侧梯度变化趋于0， 这会导致在Back propagation的时候梯度消失，也就是说收敛越来越慢 。 $Tanh’=1-tanh^2$ 可能是前面进行的操作与线性函数效果相同？为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作$(y=scale*x+shift)$，每个神经元增加了两个参数scale和shift参数，通过训练学习得到， 这样等价于非线性函数的值从正中心周围的线性区往非线性区动了动。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。（来源3） 这种表示会对模型的收敛有帮助，但是也可能破坏已经学习到的特征。为了解决这个问题，BN添加了两个可以学习的变量和 用于控制网络能够表达直接映射，也就是能够还原BN之前学习到的特征。（来源4） 整个流程就是","link":"","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://Xpangz.github.io/tags/神经网络/"}]},{"title":"人性的弱点——戴尔.卡耐基（陶曚译）","date":"2019-11-05T02:52:00.000Z","path":"2019/11/05/人性的弱点——戴尔.卡耐基（陶曚译）/","text":"人际关系的基本技巧[ 不要批评，不要指责，不要抱怨 ] “三十年前。我就懂得抱怨是愚蠢的行为。克服自己的种种缺点就已经够我忙的了，哪还有精力去抱怨上帝为何不把天赋平分给每个人”。 批评是无用的，它激起抵触，让人急于辩白；批评是危险的，它伤害自尊，甚至让人萌生恨意。 指责如同回旋镖，总会伤及自身。 “你们不要论断人，免得被人论断”。（林肯） 【如果寄出这封信，我固然发泄了自己的感受，但米德一定会辩白，甚至反而质疑我。这样不仅会引发不愉快，损害他作为指挥官的威信，甚至有可能逼得他无路可退，愤而辞职】 改变自己远比改变他人令你收益更多，并且风险更小。孔子曾言：“苟正其身矣，于从政乎何有？不能正其身，如正人何？”。 人并非理性生物。他们由情感驱使，被偏见支配，傲慢与虚荣是他们的动力之源。 批评、指责和抱怨是蠢材与生俱来的“才能”；理解和宽容却是对人品和自律的极大考验。 “伟人的伟大之处，从他对待小人物的态度中可见一斑”。（卡莱尔） 尝试以理解代替指责，设身处地去想一想对方为何如此——这比简单的批评要有益得多，也有趣得多。 [ 真心实意地感谢他人、赞美他人 ] 让他人为你效劳的方法——让对方心甘情愿地自发去做。 若想请你为我做事，唯一的方法就是给你想要的。 人性中最深层的动力是“对重视的渴求”。 如果你愿意告诉我什么事情最让你有满足感，我就能知道你是怎样的人。 为了博得同情和关注，人们有时会假装弱小，从而获得存在感。 “我的能力在于激发周围人的热情，我拥有的最大资本，是赞赏和鼓励，我以此方式激发人们的潜能。没有什么比上司的批评更打击一个人的积极性了。我从未批评过任何人。我认为给人们正面激励对工作至关重要，所以我喜欢鼓励他人，不愿意吹毛求疵。如果我看到任何闪光点，我会由衷地赞许，不吝啬赞美之词”。 我们总是对伴侣的付出习以为常，却忘记了他们也需要被称赞、被感激。 人人都知道如果连续六天不让家人或雇员进食就等同于犯罪，却连续六天、六周、甚至六十年地吝啬他们真诚称赞，他们忽略了这一点——人们像渴求食物一样渴求被认同。 除了对自尊的滋养，我别无他求。 真挚的赞美能够改变人的一生。 阿谀奉承当然注定失败。明辨是非的人一眼就能看穿其肤浅与虚伪。当然，也有些人恰恰相反，对赞美的渴求往往会令其不辨真假照单全收，正如久饿之人会饥不择食一样。 长远来看，谄媚对人际关系的危害远大于一时的成效。虚情假意的奉承话如同伪钞，一经使用，定会惹祸上身。 赞美和奉承，前者真诚，后者虚伪；前者无私，后者自私；前者发自肺腑，后者流于表面；前者为世人所歌颂，后者为世人所不齿。 普天之下的每一颗心，都会因他人的赞许而欢愉。 伤害人们感情的做法不值得提倡，也不会令对方有任何改变。 此生之路，我将走过；走过这一次，便再也无法重来。所有力所能及的善行，所有充盈于心的善意，我将毫不吝惜，即刻倾予。我将再不拖延，再不淡漠，只因此生之路，再也无法重来。 我遇见的每个人都必定在某一方面优胜于我。正因如此，我向每个人学习。（爱默生）他们的学习之处也即赞美之处。 [ 激发他人的需求 ] 所以普天之下唯一能够影响他人的方式，就是找出他们想要什么，并且交给他们如何获得。 行为根植于人类的根本欲望，无论在商界、政界、还是家庭或学校，说服别人的首要途经，是引发对方的强烈欲求。 想要影响他人，唯一的方法是以对方的需求为出发点。 在开口之前，先问问自己：“怎样才能让对方主动自发地去做这件事？”。 “洞悉他人的立场，并能够同时兼顾自己和他人的立场。” 了解对方的需求，从而激发对方的兴趣，并不等同于操纵他人、损人利己。沟通的宗旨是双赢。 表现自我是人性的重要需求。 赢得他人喜爱的六个方式[ 广受欢迎的奥秘 ] 发自内心地关注他人，不出两个月，你就能交到许多朋友；只想博得他人的关注，哪怕两年，你也交不到朋友。 人们真正关心的只有自己，这一点永远不会变，无论今天还是明天，无论现在还是未来。 漠视同胞之人，生活最为艰辛，给周遭带来的伤害也最为深痛。置身于这样的个体周围，人类命运有如堕入寒冬，生机难复。 要想得到友谊，就别怕麻烦，全心全意地为他人做些事情吧——哪怕要为此付出时间、精力、慷慨与体贴。 “我们对他人的兴趣，以他人对我们的兴趣为前提”。 双方的兴趣，皆以“真”为前提。 [ 微笑 ] 人们只有真心热爱自己在做的事，才有可能成功。 “人们往往认为感觉先于行为，但实际上，感觉与行为是同步的。相较于情绪，个人意志对行为的影响更为直接，只要调整行为，我们就能够间接地调整情绪。” “事无优劣，思想使然。”（莎士比亚） “人们的幸福感取决于他们的心境。”（林肯） “走出家门的时候，请收紧下颔，高昂起头，让空气充满肺腑，让阳光沐浴身心。向朋友问好时记得微笑，握手的时候请真心诚意。不要害怕误解，也不要在敌人身上浪费一分一秒。 请在脑海中牢牢记住你的心愿，径直向目的地前进。专注于你的伟大前程，你就会发现在日升月落之间，你已经不知不觉地抓住了每一次接近目标的机会，如同小小的珊瑚虫从澎湃海浪中汲取它需要的养分。请在脑海中仔细勾勒理想的形象，笃定的想法将帮助你一步步成长为那个美好真诚、才华横溢的自己。思想是至高无上的。请端正心态，学会无畏、坦率、乐观，因为正确的思维是创造一切的前提。心想是事成之因，虔诚的祷告者必将得到回应。倘若你心意坚定，你就不会迷失方向。收紧下颔，高昂起头——你将会成为自己的神。”(阿尔伯特.哈伯德) 它分文不取，却价值连城；它使人富有，亦于己无妨；它发生于瞬息之间，却令人永难忘怀；它使贫者丰足，令富者匮乏；它为家庭带来欢愉，为事业营造机遇，为友谊立下盟约；它是疲倦者的休憩，是忧虑者的良方，是绝望者的黎明，是哀戚者的阳光；它无法用金钱交换，无法经乞求而得，无法借与他人，也无法被人窃取；只有在给予之时，它才有存在的价值。 [ 无论对于何人，无论以何种语言，自己的名字都是世界上最甜蜜最重要的词汇 ] 人们对自己名字的在意程度，远胜于地球上其它所有名字的总和。能够熟稔地叫出对方姓名本身已是有效而不着痕迹的赞美。 每个人都希望自己的名字永垂不朽，为此付出任何代价都在所不惜。 好的习惯建立在日复一日的琐碎牺牲之上。 通过记住对方名字让对方感到被重视。 每个姓名都独立而完整地归属于特定的个体，而非其它任何人。名字令我们与众不同，成为独一无二的自己。沟通中一旦道出对方姓名，我们所传递的信息或是请求就增添了一层特别的色彩。 [ 专注地倾听，鼓励他人谈论自己 ] 她对我的旅行经历并不感兴趣，真正令她兴趣盎然并自我感觉良好的，是有人愿意倾听她的经历。 专注的倾听是我们能够给予他人的最高赞许。 关注是最含蓄的谄谀。极少有人对他人一心一意的关注无动于衷。 商务往来并无制胜之道可言，然而在对方说话的时候专注地倾听，是令对方解除戒备的最佳方式。 人们请医生有时只是需要一个听众而已。 假如你立志要能言善辩，请先学会专注聆听。做一个有趣的人，并对他人感兴趣。问对方乐于回答的问题，鼓励他们谈论自己的经历。 请记住，你的谈话对象并不关心你和你的问题，而对他们自己、他们的欲望和烦恼要感兴趣得多。他的牙疼远比异国饿殍遍地的饥荒更重要，他脖子上的疖子也远比非洲的四十次地震更让人心烦。所以下次开口之前，请先想想这点。 [ 谈论对方感兴趣的事情 ] 谈论对方最在乎的事情，是直抵对方内心深处的捷径。 “我知道，如果不是一开始就找到了对方感兴趣的话题，借此拉近距离，他不会如此平易近人。” 谈论对方的兴趣能够带来双赢。 [ 真心实意地让对方知道他有多重要 ] 我想从他那儿得到什么！！！我想从他那得到什么！！！如果人人都如此自私阴暗，付出一点小小的善意和一句真诚的称赞都所求回报；如果我们的灵魂渺小如尘沙，那么失败是注定应得的惩罚。我对邮局的那个小伙子别无他求，只想要一个无价之宝，我也确实得到了——我得到了付出而不求回报的美好感受。 令他人感到重要！ “无论何时，你们愿意人怎样对待你们，你们也要怎样对待人”（耶稣） 由衷地赞许，从不吝啬赞美之词，使人听到真诚的鼓励，而不是廉价虚伪的奉承话。 “抱歉给您添麻烦了……”“劳驾您……”“能不能请您……”“您介不介意……”诸如此类的礼貌用语如同乏味生活的润滑剂，也是教养的体现。 真相是赤裸裸的——每个人心里都认为他在某一方面比你强。走进他们内心得方式，是不动声色地让对方知道，你真心觉得他们很重要。 每个人都必定在某一方面胜于我，因此我向所有人学习（爱默生）三人行，必有我师焉（孔子） 和对方谈谈他们自己，对方听上几个小时也不会厌倦。 如何让他人想你之所想[ 赢得争论的方法只有一个，那就是避免争论 ] 为什么一定要分个胜负呢？这样会让他对你有好印象吗？为什么就不能给他留点面子呢？人家又没有问你的意见——他根本不需要你的意见。你又何必一定要和他争？别总是自己往枪口上撞。 普天之下，赢得争论的方法只有一个，那就是避免争论。 在大多数情况下，争论非但不会令双方和解，反而会火上浇油，令双方更加坚信自己言之有理。 争辩、抱怨和反驳或许会带来暂时的胜利，但你永远无法通过这表面上的胜利赢得对方的尊敬。 也许你是对的，也许你总能驳倒别人，但是那又有什么意义呢。无论输赢，你都无法改变他人的想法。 若以争吵对抗误解，怨恨则无休无止。唯有得体的处事能力、怀柔技巧和同理心才能够化解争执。 “有远大志向的人不会把时间浪费在无益的争执上”“因为意气用事毫无意义。你的私事再大也大不过天，所以在个人问题上要懂得让步。有狗拦路的时候，最好给它让道，而不要为了争路而被它反咬一口；若是被咬了，即使你杀了它，你的伤口也不会马上愈合。” 如何避免分歧升级为争吵 12345678接受分歧。请记住这句话：“如果两个合作伙伴总是意见一致，那么其中一个人就没有存在的意义。”如果对方提出了你从未想到的观点，请心存感激。不同的意见或许会帮助你避免犯错。不要纵容直觉反应。面对不利处境，人类的本能使我们下意识地进入戒备模式。这时请格外注意。保持冷静，警锡你的本能反应，一它会使你成为最槽的那个你，而非状态最好的那个你。控制情绪。观察对方是否易怒，你就能知道他是君子还是小人。先听，后说。给对方说话的机会，听他们把话说完。反驳和争辩只会徒增隔阂。建立沟通的桥梁，不要筑起误解的壁垒。求同存异。听到对方的看法之后，请先想想你认可的部分。诚恳。在能够让步的时候让步，在应该认错的时候认错。这会令对方放下戒心，从而减少摩擦。向对方承诺你会认真考虑他的想法，并且说到做到。对方很有可能是正确的。借这个机会深思熟虑，总好过事后被对方指责“我们告诉过你，可你就是不听”。真心诚意地感谢对方的重视。对方愿意花时间和你争辩，是因为他和你对同一件事感兴趣。将他们视为真心愿意帮助你的人，也许就能化敌为友。给双方足够时间找出症结所在，不要急于采取行动。主动建议推迟讨论时间，将所有的细节都考虑清楚。再次交涉之前，请坦诚地问问自己这些关键问题：对方有没有可能是正确的，或是部分正确？他的论点中是否有值得肯定的地方？我的建议能够解决问题，还是只会引发不快？我的行为是会把对方推向对立面，还是拉近我们的关系？我的决策是否让人们更尊重我？我会赢，还是会输？如果我赢了，我会付出什么代价？如果我保持缄默，纷争是否会就此平息？目前的局面对我而言，是否意味着机会？","link":"","tags":[{"name":"阅读摘抄感悟","slug":"阅读摘抄感悟","permalink":"http://Xpangz.github.io/tags/阅读摘抄感悟/"}]},{"title":"浅析Unet论文","date":"2019-10-27T14:00:00.000Z","path":"2019/10/27/浅析Unet网络/","text":"首先，论文地址。 特点： fast the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. 没有任何全连接层，只使用了每层卷积的有效部分。i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. The resulting network is applicable to various biomedical segmentation problems. 相关研究分析：CNN有局限性，在于网络和训练集的大小。自ImageNet之后，更大更深的网络也相继出现。生物医学无法提供数以千计的训练图像。 A classifier output that takes into account the features form multiple layers,使得 good localization 和good use of context是同时可能的。 Network​ 两条路径，一条contracting path to capture context ,一条expanding path that enable precise localization。 contracting path是经典的CNN网络。最后一层使用1x1 convolution 将64 component feature vector映射到所需类别数量。 总共23个卷积层。 为了实现分割后输出图的无缝平铺，选择输入tile size的大小很重要(例如以供所有的2x2池化操作都能应用到有x与y均匀大小的层当中) 无填充卷积unpadded convolutions：即卷积pad=0,这使得图像进行一次卷积会失去边界（图像一周）像素点。因此在3中对应path的feature连接时需要对contract path进行crop。因此The cropping 是必要的，因为每次卷积操作中边界的pixel有损失。 如何crop:收缩路径和扩展路径中的对应层通过channel维度连接（concatenation）起来。（与fcn中的不同）输出结果应包括localization,每个pixel都要被贴上标签。high resolution features from thr contracting path are combined with the upsampled output.卷积层就能够基于这些combined后的信息学习到更精确的输出。 在upsampling part 有大量的特征通道，能够 allow the networkto propagate context information to higher resolution layers 通过overlap-tile策略允许任意大的图片进行无缝分割。 tiling strategy is important to apply the network to large images(since otherwise the resolution would be limited by the GPU memory,???)——缺失边界信息的pixel处理：使用镜像 对现有训练集使用弹性形变(elastic deformations)进行数据增广，好处：1、通过弹性形变网络能够学到invariance；2、without the need to see these transformations in the annotated image corpus(带注释的图像语料库) 因为形变是组织中最常用的变化，弹性形变能够很有效的模拟这种变化。 相同类别且很接近的目标分离：提出一种权重损失，使得相互touching的目标之间获得较大的权重(where the separating background labels between touching cells obtain a large weight in the loss function.),强迫网络学习边界信息。 Training输入图像和相应分割图像常被用来训练网络，在Caffe中使用 stochastic gradient descent来实现。 由于unpadded convolutions，输出图像小于一个恒定边界宽度的输入图像。为了最小化开销和最大限度地使用GPU，我们倾向于large input tiles over a large batch size and hence reduce the batch to a single image.（为了对于一张图片减少批次处理，倾向于很大卷积在大规模的批次上）。 文章相应地使用high momentum(0.99)以便先前所见到的大量训练样本决定当前优化步骤的更新。使用Momentum为解决学习率小时收敛到极值的速度较慢，而学习率较大时，又容易发生震荡，以及Hessian矩阵病态条件问题，可以理解为惯性， 积攒了历史的梯度 ，使当前梯度小幅影响优化方向，而不是完全决定优化方向。也起到了减小波动的效果。 momentum公式（参数为lambda，越大表示之前梯度对现在方向的影响也越大）：$$v_t = \\lambda v_{t-1}+\\eta*\\nabla_{\\theta}J(\\theta)\\\\theta = \\theta - v_t$$能量函数是在最终特征图上使用soft-max+交叉熵损失函数。soft-max为$a_k(x)$表示在特征通道k中像素位置的激活，K是标签类别数量，$p_k(x)$是approximated maximum-function,即最大的$a_k(x)$对应的p(x)就接近于1，其余接近于0）：$$p_k(x)=\\dfrac{e^{a_{k}(x)}}{\\sum^K_{k’=1}e^{a_{k’}(x)}}$$Cross entropy loss function为：$$E = \\sum_{x\\in \\Omega}\\omega(x)log(p_{l(x)}(x))$$交叉熵通过在每个位置上$p_{l(x)}(x)$距离1的违背程度进行惩罚，其中$l$是每个像素真正的标签，$\\omega$是在训练中给pixels定义重要性所引入的权重。 预先计算每个真正分割的权重图for each ground truth segmentation,来补偿在训练集中每个特定标签的不同频率？并且强迫网络学习相互接触的细胞之间的分割。 边界上的计算——morphological operations。权重计算公式： 一个好的初始权重极其重要，理想地，应调整初始权重，使得网络中的每个特征映射都有近似单位方差。本文从标准方差($\\sqrt{\\dfrac{2}{N}}$,N=3x3x64=576)的高斯分布中提取初始权值。 数据增广：主要需要平移和旋转的不变性以及形变和灰度值变化的鲁棒性。训练样本的随机弹性形变似乎是训练一个标注图像很少的分割网络的关键概念。Unet中：我们使用3×3粗网格上的随机位移向量生成平滑形变。位移是从一个10像素标准差的高斯分布中采样的。Per-pixel displacements are then computed using bicubic interpolation.在contracting path最后的Drop-out layers 执行进一步的implicit data augmentation. 其它价值文献博客： The value of data augmentation for learning invariance has beenshown in Dosovitskiy et al. [Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative un- supervised feature learning with convolutional neural networks. In: NIPS (2014)] in the scope of unsupervised feature learning. 深度学习优化器总结","link":"","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://Xpangz.github.io/tags/神经网络/"},{"name":"论文分析","slug":"论文分析","permalink":"http://Xpangz.github.io/tags/论文分析/"}]},{"title":"CNN 手写识别代码笔记","date":"2019-10-25T08:00:00.000Z","path":"2019/10/25/CNN 手写识别笔记/","text":"深入学习卷积神经网络（CNN）的原理知识 代码分析:数据导入：123import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True) input_data该函数专门用于下载mnist数据，我们直接调用就可以了，该函数执行完成后，会在当前目录下新建一个文件夹MNIST_data,下载的数据将放入这个文件夹内。 定义函数（变成变量、先定义好卷积层和池化层的大小步长）1234567891011def weight_variable(shape): return tf.Variable(tf.truncated_normal(shape,stddev=0.1))def bias_vairable(shape): return tf.Variable(tf.constant(0.1, shape=shape))def conv2d(x,W): return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padd 要用tf.Variable定义，它才能成为变量 tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。它是从截断的正态分布中输出随机值，虽然同样是输出正态分布，但是它生成的值是在距离均值两个标准差范围之内的，也就是说，在tf.truncated_normal中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择。这样保证了生成的值都在均值附近。 tf.constant(value,dtype=None,shape=None,name=&#39;Const&#39;,verify_shape=False)创建一个常量tensor,按照value来赋值，value可以是个数或list，为一个数时，那么该常量中的所有值都用该数来赋值， 而当是一个列表时，注意列表的长度必须小于等于第三个参数shape的大小（即各维大小的乘积） ， 如果列表大小小于shape大小，则会用列表的最后一项元素填充剩余的张量元素。第五项如果修改为True的话表示检查value的形状与shape是否相符，如果不符会报错。 tf.nn.xx请参看W3Cschool tf.Variable 一定要定义成变量它才是变量 定义批次大小、要用到变量定义占位符、和修改图像形状12345678batch_size = 100 # 数据处理，批次大小n_batch = mnist.train.num_examples // batch_size # 批次数量x = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob = tf.placeholder(tf.float32)x_image = tf.reshape(x,[-1,28,28,1]) placeholder(dtype, shape=None, name=None) dtype：数据类型。常用的是tf.float32,tf.float64等数值类型shape：数据形状。默认是None，就是一维值，也可以是多维，比如[None, 3]表示列是3，行不定 。规定输入结构。name：名称。特点： 先类似于变量占个位置，之后sess.run()时需要从外界传入值来代替它，如果用这个的话就意味着在sess.run时输入一个值 tf.reshape(tensor,shape, name=None) 将tensor 变换为参数shape的形式，-1代表无需自己指定，函数会自动计算 第一层卷积和池化12345678910W_conv1 = weight_variable([5,5,1,32]) # 5x5的采样窗口，32个卷积核从1个平面抽取特征b_conv1 = bias_vairable([32]) #每个卷积核一个偏置值#28x28x1 的图片卷积之后变为28x28x32h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # relu(x) = max(x,0)#池化之后变为 14x14x32h_pool1 = max_pool_2x2(h_conv1) 定义了卷积核，核上的权重是随机的。bias 它其实就是函数的截距，与线性方程 y=wx+b 中的 b 的意义是一致的 ，不设置分类就经过原点，受到了限制。 卷积公式类似于：激活函数(W*x+b),只不过里面的乘法使用的是卷积运算。这种运算的具体细节被tensorflow隐藏了。同理池化也一样。 第二层卷积池化1234567#第二次卷积之后变为 14*14*64W_conv2 = weight_variable([5,5,32,64])b_conv2 = bias_vairable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)#第二次池化之后变为 7x7x64h_pool2 = max_pool_2x2(h_conv2) 全连接层1234567#第一个全连接层W_fc1 = weight_variable([7*7*64,1024]) #此处的7764是7x7x64，*被隐藏了b_fc1 = bias_vairable([1024])#7x7x64的图像变成1维向量h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 这里将第二次池化后的图像扁平化(一维化)了,把7x7x64变成了1024个全连接神经元。 tf.nn.dropout(X,keep_prob)主要含两个变量，输入张量X和 概率值keep_prob(也是要以张量输入),float类型，每个元素被保留下来的概率。 是为了防止过拟合而随机抑制神经元 ，一般用在全连接层。 Dropout就是在不同的训练过程中随机扔掉一部分神经元。也就是让某个神经元的激活值以一定的概率p，让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算。但是它的权重得保留下来（只是暂时不更新而已），下次样本输入时它可能又得工作了。 第二个全连接层1234W_fc2 = weight_variable([1024,10])b_fc2 = bias_vairable([10])logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2prediction = tf.nn.sigmoid(logits) 最后预测用的激活函数是sigmoid函数，图像为 训练和计算精度1234567loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 采用Adam优化算法prediction_2 = tf.nn.softmax(prediction)correct_prediction = (tf.equal(tf.argmax(prediction_2,1), tf.argmax(y,1))) # 得到bool型变量，axis=1的时候，将每一行最大元素所在的索引记录下来，最后返回每一行最大元素所在的索引数组。accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) #tf.cast将bool型变量变成数， tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes第二个参数labels：实际的标签，大小同上。第一步是先对网络最后一层的输出做一个softmax，这一步通常是求取输出属于某一类的概率$$softmax(x)i = \\dfrac{exp(x_i)}{\\sum{j}exp(x_j)}$$ 第二步是softmax的输出向量[Y1，Y2,Y3…]和样本的实际标签做一个交叉熵，公式如下：$$H_{y’}(y) = -\\sum_iy’{_i}\\log(y_i)$$其 指代实际的标签中第i个的值(用mnist数据举例，如果是3，那么标签是[0，0，0，1，0，0，0，0，0，0]，除了第4个值为1，其他全为0）就是softmax的输出向量[Y1，Y2,Y3…]中，第i个元素的值显而易见，预测越准确，结果的值越小（别忘了前面还有负号），最后求一个平均，得到我们想要的loss。 注意！！！这个函数的返回值并不是一个数，而是一个向量，如果要求交叉熵，我们要再做一步tf.reduce_sum操作,就是对向量里面所有元素求和，最后才得到，如果求loss，则要做一步tf.reduce_mean操作，对向量求均值！ tf.argmax(input,axis) 根据axis取值的不同返回每行或者每列最大值的索引。 axis=0时比较每一列的元素，将每一列最大元素所在的索引记录下来，最后输出每一列最大元素所在的索引数组 ,axis=1时，就是比较每一行的元素。 tf.equal是比较两个元素是否相当，相等为1，否则为0。 最后运行：12345678with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(21): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step, feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;) acc = sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0&#125;) print(\"Iter: \" + str(epoch) + \", acc: \" + str(acc)) tf.Session() 1234tf.Session.run(xx) 执行这个结构xx# 如果使用下述结构，自动带sess.close.with tf.Session() as sess: ......sess.run()...... 定义了变量后一定要用的最重要一步：tf.initialize_all_variables() ，要初始化所有的变量，才会在tensorflow中激活，用tf.sess.run() 因为有tf.placeholder，所以sess.run()中要多添加一项，即给出输入的值，用feed_dict={}传入，类似字典，有名称和值。 为什么要用激励函数？引入非线性，可以自己创建激励函数，但前提是要保证它是可微分的(反向传播时用)。少量神经层中，可以尝试各种激励函数，卷积层中推荐使用RELU激励函数，循环神经网络中推荐RELU,tanh。 疑问？ 边缘效应 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True)batch_size = 100n_batch = mnist.train.num_examples // batch_sizedef weight_variable(shape): return tf.Variable(tf.truncated_normal(shape,stddev=0.1)) # 它是从截断的正态分布中输出随机值def bias_vairable(shape): return tf.Variable(tf.constant(0.1, shape=shape)) # 创建一个常量tensor,按照value来赋值def conv2d(x,W): return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')x = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob = tf.placeholder(tf.float32)x_image = tf.reshape(x,[-1,28,28,1])W_conv1 = weight_variable([5,5,1,32]) # 5*5的采样窗口，32个卷积核从1个平面抽取特征b_conv1 = bias_vairable([32]) #每个卷积核一个偏置值# 28*28*1 的图片卷积之后变为28*28*32h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # relu(x) = max(x,0)# 池化之后变为 14*14*32h_pool1 = max_pool_2x2(h_conv1)# 第二次卷积之后变为 14*14*64W_conv2 = weight_variable([5,5,32,64])b_conv2 = bias_vairable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)# 第二次池化之后变为 7*7*64h_pool2 = max_pool_2x2(h_conv2)# 第一个全连接层W_fc1 = weight_variable([7*7*64,1024])b_fc1 = bias_vairable([1024])# 7*7*64的图像变成1维向量h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# 第二个全连接层W_fc2 = weight_variable([1024,10])b_fc2 = bias_vairable([10])logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2prediction = tf.nn.sigmoid(logits)loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))train_step = tf.train.AdamOptimizer(0.001).minimize(loss)prediction_2 = tf.nn.softmax(prediction)correct_prediction = (tf.equal(tf.argmax(prediction_2,1), tf.argmax(y,1)))accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(21): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step, feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:0.7&#125;) acc = sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0&#125;) print(\"Iter: \" + str(epoch) + \", acc: \" + str(acc))","link":"","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://Xpangz.github.io/tags/神经网络/"},{"name":"代码分析","slug":"代码分析","permalink":"http://Xpangz.github.io/tags/代码分析/"}]},{"title":"Markdown初学者入门指南","date":"2019-10-17T03:08:14.000Z","path":"2019/10/17/Markdown初学者入门指南/","text":"照搬Markdown语法图文全面详解(10分钟学会)，用作备份。 更详细教程 前言写过博客或者github上面的文档的，应该知道Markdown语法的重要性，不知道的朋友们也别着急，一篇博客轻松搞定Markdown语法。话说这个语法超级简单，一看就会，不信你点进来看看。 1. 快捷键 功能 快捷键 加粗 Ctrl + B 斜体 Ctrl + I 引用 Ctrl + Q 插入链接 Ctrl + L 插入代码 Ctrl + K 插入图片 Ctrl + G 提升标题 Ctrl + H 有序列表 Ctrl + O 无序列表 Ctrl + U 横线 Ctrl + R 撤销 Ctrl + Z 重做 Ctrl + Y 2. 基本语法2.1 字体设置斜体、粗体、删除线这里是文字这里是文字这里是文字这里是文字这里是文字 2.2 分级标题写法1： # 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 这个写法和 文字效果是一样的 写法2： 这是一个一级标题或者 二级标题- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 2.3 链接（1）插入本地图片链接语法规则，有两种写法：注意：这个图片描述可以不写。 示例图如下： （2）插入互联网上图片语法规则：注意：这个图片描述可以不写。 示例如下：（3）自动连接Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。也可以直接写，也是可以显示成链接形式的例如： 2.4 分割线你可以在一行中用三个以上的星号(*)、减号(-)、底线(_)来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。 2.5 代码块对于程序员来说这个功能是必不可少的，插入程序代码的方式有两种，一种是利用缩进(tab), 另一种是利用英文“`”符号（一般在ESC键下方，和~同一个键）包裹代码。 （1）代码块：缩进 4 个空格或是 1 个制表符。效果如下：（2）行内式：如果在一个行内需要引用代码，只要用反引号`引起来就好（一般在ESC键下方，和~同一个键） （3）多行代码块与语法高亮：在需要高亮的代码块的前一行及后一行使用三个单反引号“`”包裹，就可以了。示例如下： （4）代码块里面包含html代码在代码区块里面， &amp; 、 &lt; 和 &gt; 会自动转成 HTML 实体，这样的方式让你非常容易使用 Markdown 插入范例用的 HTML 原始码，只需要复制贴上，剩下的 Markdown 都会帮你处理。 注意：简书代码块里不支持html。 示例如下： 2.6 引用在被引用的文本前加上&gt;符号，以及一个空格就可以了，如果只输入了一个&gt;符号会产生一个空白的引用。 （1）基本使用使用如下图所示： （2）引用的嵌套使用使用如图所示：（3）引用其它要素引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等。使用如图所示： 2.7 列表（1）无序列表使用 *，+，- 表示无序列表。注意：符号后面一定要有一个空格，起到缩进的作用。 （2）有序列表使用数字和一个英文句点表示有序列表。注意：英文句点后面一定要有一个空格，起到缩进的作用。 （3）无序列表和有序列表同时使用 （4）列表和其它要素混合使用列表不光可以单独使用，也可以使用其他的 Markdown 语法，包括标题、引用、代码区块等。 注意事项： （1）加粗效果不能直接用于列表标题里面，但是可以嵌套在列表里面混合使用。（2）列表中包含代码块（前面加2个tab或者8个空格，并且需要空一行，否则不显示）。.使用示例如下图： （5）注意事项在使用列表时，只要是数字后面加上英文的点，就会无意间产生列表，比如2017.12.30 这时候想表达的是日期，有些软件把它被误认为是列表。解决方式：在每个点前面加上\\就可以了。如下图所示： 2.8 表格表格的基本写法很简单，就跟表格的形状很相似： 表格对齐方式：我们可以指定表格单元格的对齐方式，冒号在左边表示左对齐，右边表示有对齐，两边都有表示居中。 如下图所示： 3. 常用技巧3.1 换行方法1: 连续两个以上空格+回车方法2：使用html语言换行标签： 3.2 缩进字符不断行的空白格 或 半角的空格 或 全角的空格 或 3.3 特殊符号（1）对于 Markdown 中的语法符号，前面加反斜线\\即可显示符号本身。 示例如下： （2）其他特殊字符，示例如下：想知道字符对应的Unicode码，可以看这个网站：https://unicode-table.com/cn/ 附上几个工具对特殊字符的支持的对比图: 3.4 字体、字号与颜色Markdown是一种可以使用普通文本编辑器编写的标记语言，通过类似HTML的标记语法，它可以使普通文本内容具有一定的格式。但是它本身是不支持修改字体、字号与颜色等功能的！ CSDN-markdown编辑器是其衍生版本，扩展了Markdown的功能（如表格、脚注、内嵌HTML等等）！对，就是内嵌HTML，接下来要讲的功能就需要使用内嵌HTML的方法来实现。 字体，字号和颜色编辑如下代码 具体颜色分类及标记请看下表： 3.5 链接的高级操作链接的高级操作（这个需要掌握一下，很有用） 1.行内式这个在上文第二条基本语法的 链接这个小节已经过，这里就不继续讲解了。 2.参考式链接在文档要插入图片的地方写![图片或网址链接][标记]，在文档的最后写上[标记]:图片地址 “标题”。（最后这个”标题”可以不填写） 示例如下： 3.内容目录在段落中填写 [TOC] 以显示全文内容的目录结构。 4.锚点锚点其实就是页内超链接。比如我这里写下一个锚点，点击回到目录，就能跳转到目录。在目录中点击这一节，就能跳过来。 注意： 在简书中使用锚点时，点击会打开一个新的当前页面，虽然锚点用的不是很舒服，但是可以用注脚实现这个功能。 语法说明： 在你准备跳转到指定标题后插入锚点{ #锚点 }，然后在文档的其它地方写上连接到锚点的链接。 使用如下图所示： 5.注脚语法说明：在需要添加注脚的文字后加上脚注名字[^注脚名字],称为加注。 然后在文本的任意位置(一般在最后)添加脚注，脚注前必须有对应的脚注名字。 示例如下：注： 脚注自动被搬运到最后面，请到文章末尾查看，并且脚注后方的链接可以直接跳转回到加注的地方。 由于简书不支持锚点，所以可以用注脚实现页面内部的跳转。 3.6 背景色Markdown本身不支持背景色设置，需要采用内置html的方式实现：借助 table, tr, td 等表格标签的 bgcolor 属性来实现背景色的功能。举例如下： 背景色是：orange 3.7 emoji表情符号emoji表情使用:EMOJICODE:的格式，详细列表可见https://www.webpagefx.com/tools/emoji-cheat-sheet/ 当然现在很多markdown工具或者网站都不支持。 下面列出几个平台的对比： 工具或网站 是否支持emoji表情符号 简书 否 MarkDownPad 否（不知道付费版是否支持） 有道云笔记 否 zybuluo.com 否 github 是 4. 高端用法4.1 Latex数学公式使用LaTex数学公式 1.行内公式：使用两个”$”符号引用公式: $公式$ 2.行间公式：使用两对“$$”符号引用公式： $$公式$$ 输入$\\sqrt{x^{2}}$显示结果是x2−−√x2 具体可以参考 markdown编辑器使用LaTex数学公式（https://link.jianshu.com/?t=http%3A%2F%2Fblog.csdn.net%2Ftestcs_dn%2Farticle%2Fdetails%2F44229085） latex数学符号详见：常用数学符号的 LaTeX 表示方法 4.2 流程图这里简单介绍一下流程图的语法，仅作为了解，如下图所示： 4.3 制作一份待办事宜—-Todo 列表 4.4 绘制 序列图 4.5 绘制 甘特图 5. Markdown工具1.markdownpad软件，就是利用markdown语言写笔记的。官网下载地址：http://markdownpad.com/ 软件安装之后的示意图如下图所示： 2.有道云笔记支持markdownpad语法。官方网址：http://note.youdao.com/ 它有在线网页版以及PC端可以下载。当然有道云笔记也支持html语法。 网页版使用markdown示例图如下： 3.在线编辑markdown https://www.zybuluo.com/mdeditor","link":"","tags":[{"name":"入门指南","slug":"入门指南","permalink":"http://Xpangz.github.io/tags/入门指南/"}]},{"title":"Jupyter Notebooks入门指南","date":"2019-10-16T08:47:10.000Z","path":"2019/10/16/Jupyter Notebooks初学者入门指南/","text":"首先奉上链接入门｜始于Jupyter Notebooks：一份全面的初学者实用指南，Jupyter notebook学习笔记，自己仅对这些文章进行了些提炼。 启动​ 打开anaconda文件夹中的jupyter，命令窗口会自动生成网络地址，将其复制粘贴到浏览器中即可。 ​ 打开笔记本后，会看到顶部 有三个选项卡：Files、Running 和 Clusters。其中，Files 基本上就是列出所有文件，Running 是展示你当前打开的终端和笔记本，Clusters 是由 IPython 并行提供的 。 ​ 要打开一个新的 Jupyter 笔记本，点击页面右侧的「New」选项。你在这里会看到 4 个需要选择的选项： Python 3 Text File （ 选择 Text File，你会得到一个空面板。你可以添加任何字母、单词和数字。其基本上可以看作是一个文本编辑器（类似于 Ubuntu 的文本编辑器）。你可以在其中选择语言（有很多语言选项），所以你可以在这里编写脚本。你也可以查找和替换该文件中的词。 ） Folder （ 选择 Folder 选项时，你会创建一个新的文件夹，你可以在其中放入文件，重命名或删除它。各种操作都可以。 ） Terminal （ Terminal 完全类似于在 Mac 或 Linux 机器上的终端（或 Windows 上的 cmd）。其能在你的网络浏览器内执行一些支持终端会话的工作。在这个终端输入 python，你就可以开始写你的 Python 脚本了！ ） 在New选项中选择Python3，会出现下面的屏幕(图片使用相对路径：/images/xxx.png) 在蓝色框上面的带图标的菜单选项为： 保存并检查、插入代码块、剪切、复制、粘贴、向上和向下移动单元、运行单元内的代码、停止代码以及重启 kernel 。 上图的代码处，下拉框有4个选项，分别使单元有不同功能： 代码， 写代码的地方。 标记， 这是写文本的地方。你可以在运行一段代码后添加你的结论、添加注释等。 原生 NBConvert， 这是一个可将你的笔记本转换成另一种格式（比如 HTML）的命令行工具。 标题， 这是你添加标题的地方，这样你可以将不同的章节分开，让你的笔记本看起来更整齐更清晰。这个现在已经被转换成 Markdown 选项本身了。输入一个「##」之后，后面输入的内容就会被视为一个标题。 Magic Function 开发人员已经插入了预定义的 magic functions，使你的工作更方便和更具交互性。你可以运行以下命令来查看这些函数的列表(注意：通常不需要输入“％”，因为通常 Automagic 是默认打开的）： %lsmagic magic command有两种运行方式： 逐行运行(Line-wise) 逐块运行(Cell-wise) Line-wise 是当你想要执行一个单行命令的时候使用，而 Cell-wise 是你想要执行的命令不仅仅是一行，而是整个单元格中的整个代码块时使用。 在逐行运行模式中，所有给定的命令都必须以 ％ 字符开始，而在逐块运行模式下，所有命令都必须以 %% 开头。 交互式仪表盘 在你考虑添加小部件之前，你需要导入 widgets 软件包： from ipywidgets import widgets 小部件的基本类型有典型的文本输入小部件、基于输入的小部件和按钮小部件。 关于小部件的完整指南，请参阅：https://blog.dominodatalab.com/interactive-dashboards-in-jupyter/ 键盘快捷键 Jupyter Notebooks 提供了两种不同的键盘输入模式——命令和编辑。命令模式是将键盘和笔记本层面的命令绑定起来，并且由带有蓝色左边距的灰色单元边框表示。编辑模式让你可以在活动单元中输入文本（或代码），用绿色单元边框表示。 进入命令模式之后（此时你没有活跃单元），你可以尝试以下快捷键： 命令模式 (按键 Esc 开启) 说明 Enter 转入编辑模式 Shift-Enter 运行本单元，选中下个单元 Ctrl-Enter 运行本单元 Alt-Enter 运行本单元在其下插入新单元 Y 单元转入代码状态 M 单元转入markdown状态 R 单元转入raw状态 1 设定 1 级标题 2 设定 2 级标题 3 设定 3 级标题 4 设定 4 级标题 5 设定 5 级标题 6 设定 6 级标题 Up 选中上方单元 K 选中上方单元 Down 选中下方单元 J 选中下方单元 Shift-K 扩大选中上方单元 Shift-J 扩大选中下方单元 A 在上方插入新单元 B 在下方插入新单元 X 剪切选中的单元 C 复制选中的单元 Shift-V 粘贴到上方单元 V 粘贴到下方单元 Z 恢复删除的最后一个单元 D,D 删除选中的单元 Shift-M 合并选中的单元 Ctrl-S 文件存盘 S 文件存盘 L 转换行号 O 转换输出 Shift-O 转换输出滚 动 Esc 关闭页面 Q 关闭页面 H 显示快捷键帮助 I,I 中断Notebook内核 0,0 重启Notebook内核 Shift 忽略 Shift-Space 向上滚动 Space 向下滚动 Shift+↑或↓ 可选择多个单元 F 会弹出「查找和替换」菜单 处于编辑模式时（在命令模式时按 Enter 会进入编辑模式），你会发现下列快捷键很有用： 编辑模式 ( Enter 键启动) 说明 Tab 代码补全或缩进 Shift-Tab 提示 Ctrl-] 缩进 Ctrl-[ 解除缩进 Ctrl-A 全选 Ctrl-Z 复原 Ctrl-Shift-Z 再做 Ctrl-Y 再做 Ctrl-Home 跳到单元开头 Ctrl-Up 跳到单元开头 Ctrl-End 跳到单元末尾 Ctrl-Down 跳到单元末尾 Ctrl-Left 跳到左边一个字首 Ctrl-Right 跳到右边一个字首 Ctrl-Backspace 删除前面一个字 Ctrl-Delete 删除后面一个字 Esc 进入命令模式 Ctrl-M 进入命令模式 Shift-Enter 运行本单元，选中下一单元 Ctrl-Enter 运行本单元 Alt-Enter 运行本单元，在下面插入一单元 Ctrl-Shift– 分割单元 Ctrl-Shift-Subtract 分割单元 Ctrl-S 文件存盘 Shift 忽略 Up 光标上移或转入上一单元 Down 光标下移或转入下一单元 Ctrl + Enter 会运行你的整个单元块 Ctrl + Shift + F 打开命令面板 要查看键盘快捷键完整列表，可在命令模式按「H」或进入「Help &gt; Keyboard Shortcuts」。你一定要经常看这些快捷键，因为常会添加新的。 Jupyter Notebooks扩展这个在安装Notebooks时似乎出错了 Installing jupyter_contrib_nbextensions 最后 最佳实践 尽管独自工作可能很有趣，但大多数时候你都是团队的一员。在这种情况下，遵循指导原则和最佳实践是很重要的，能确保你的代码和 Jupyter Notebooks 都有适当的注释，以便与你的团队成员保持一致。这里我列出了一些最佳实践指标，你在 Jupyter Notebooks 上工作时一定要遵守： 对任何程序员而言都是最重要的事情之一——总是确保你为你的代码添加了适当的注释！ 确保你的代码有所需的文档。 考虑一个命名方案并贯彻始终。这能让其他人更容易遵循。 不管你的代码需要什么库，都在你的笔记本起始处导入它们。（并在旁边添加注释说明你载入它们的目的） 确保你的代码有适当的行距。你不要将你的循环和函数放在同一行——否则如果后面要引用它们，会让人抓狂的！ 有时候你的文件中有非常大量的代码。看看能不能将你认为不重要的某些代码隐藏起来，之后再引用。这能让你的笔记本看起来整洁清晰，这是非常可贵的。 查看这个在 matplotlib 上的笔记本，看看可以如何简练地进行呈现：http://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb 另外，还会有后续补充~~","link":"","tags":[{"name":"入门指南","slug":"入门指南","permalink":"http://Xpangz.github.io/tags/入门指南/"}]},{"title":"postname","date":"2019-10-09T08:50:19.000Z","path":"2019/10/09/postname/","text":"一、初次写文章 要学会用Markdown来写博客，自己的路还有很长要走 什么是Markdown？","link":"","tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://Xpangz.github.io/tags/Hexo/"}]}]